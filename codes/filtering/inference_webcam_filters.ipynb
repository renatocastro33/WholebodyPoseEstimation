{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import cv2\n",
    "import copy \n",
    "sys.path.insert(1, '../../src/')\n",
    "import matplotlib.pyplot as plt\n",
    "from wholebodypose.models.rtmpose.model import RTMPoseModel\n",
    "from wholebodypose.utils.vision import DrawerPose\n",
    "draw_skeleton = DrawerPose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model to /opt/conda/lib/python3.10/site-packages/mediapipe/modules/pose_landmark/pose_landmark_heavy.tflite\n"
     ]
    }
   ],
   "source": [
    "from wholebodypose.models.mediapipe.model import MediapipeModel\n",
    "mode_coco = True\n",
    "kpt_thr = 0.5\n",
    "model = MediapipeModel(mode_coco=mode_coco,use_thresholding=False,kpt_thr=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install onnxruntime-gpu==1.17.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install onnxruntime==1.17.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip uninstall -y onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip uninstall -y onnxruntime-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install numpy==1.23.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip uninstall -y numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.23.4'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "numpy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.11.0'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "cv2.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!rm -r /root/.cache/rtmlib/hub/checkpoints/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.openmmlab.com/mmpose/v1/projects/rtmposev1/onnx_sdk/yolox_m_8xb8-300e_humanart-c2c7a14a.zip\" to /root/.cache/rtmlib/hub/checkpoints/yolox_m_8xb8-300e_humanart-c2c7a14a.zip\n",
      "100% 89.9M/89.9M [00:35<00:00, 2.69MB/s]\n",
      "\u001b[0;93m2025-07-24 05:13:09.664561483 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2025-07-24 05:13:09.664727235 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n",
      "Downloading: \"https://download.openmmlab.com/mmpose/v1/projects/rtmw/onnx_sdk/rtmw-dw-x-l_simcc-cocktail14_270e-384x288_20231122.zip\" to /root/.cache/rtmlib/hub/checkpoints/rtmw-dw-x-l_simcc-cocktail14_270e-384x288_20231122.zip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load /root/.cache/rtmlib/hub/checkpoints/yolox_m_8xb8-300e_humanart-c2c7a14a.onnx with onnxruntime backend\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 204M/204M [00:52<00:00, 4.10MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load /root/.cache/rtmlib/hub/checkpoints/rtmw-dw-x-l_simcc-cocktail14_270e-384x288_20231122.onnx with onnxruntime backend\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2025-07-24 05:14:09.759444059 [W:onnxruntime:, graph.cc:3593 CleanUnusedInitializersAndNodeArgs] Removing initializer '1701'. It is not used by any node and should be removed from the model.\u001b[m\n",
      "\u001b[0;93m2025-07-24 05:14:09.759599805 [W:onnxruntime:, graph.cc:3593 CleanUnusedInitializersAndNodeArgs] Removing initializer '1706'. It is not used by any node and should be removed from the model.\u001b[m\n",
      "\u001b[0;93m2025-07-24 05:14:09.759816268 [W:onnxruntime:, graph.cc:3593 CleanUnusedInitializersAndNodeArgs] Removing initializer '1709'. It is not used by any node and should be removed from the model.\u001b[m\n",
      "\u001b[0;93m2025-07-24 05:14:10.130415199 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2025-07-24 05:14:10.130567644 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n"
     ]
    }
   ],
   "source": [
    "#model = RTMPoseModel(mode='performance',backend='onnxruntime', device='cpu',use_thresholding=True,filter_noise=True,kpt_thr=2.5)\n",
    "model = RTMPoseModel(mode='performance',backend='onnxruntime', device='cuda',use_thresholding=True,filter_noise=True,kpt_thr=2.5)\n",
    "kpt_thr = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;32m17165.mp4\u001b[0m*       \u001b[01;32mGIVE_24651.mp4\u001b[0m*  \u001b[01;32mGIVE_69343.mp4\u001b[0m*\n",
      "\u001b[01;32mGIVE_24649.mp4\u001b[0m*  \u001b[01;32mGIVE_24660.mp4\u001b[0m*  \u001b[01;32mREADME.md\u001b[0m*\n"
     ]
    }
   ],
   "source": [
    "ls ../../data/videos/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnt: 1\n",
      "cnt: 2\n",
      "cnt: 3\n",
      "cnt: 4\n",
      "cnt: 5\n",
      "cnt: 6\n",
      "cnt: 7\n",
      "cnt: 8\n",
      "cnt: 9\n",
      "cnt: 10\n",
      "cnt: 11\n",
      "cnt: 12\n",
      "cnt: 13\n",
      "cnt: 14\n",
      "cnt: 15\n",
      "cnt: 16\n",
      "cnt: 17\n",
      "cnt: 18\n",
      "cnt: 19\n",
      "cnt: 20\n",
      "cnt: 21\n",
      "cnt: 22\n",
      "cnt: 23\n",
      "cnt: 24\n",
      "cnt: 25\n",
      "cnt: 26\n",
      "cnt: 27\n",
      "cnt: 28\n",
      "cnt: 29\n",
      "cnt: 30\n",
      "cnt: 31\n",
      "cnt: 32\n",
      "cnt: 33\n",
      "cnt: 34\n",
      "cnt: 35\n",
      "cnt: 36\n",
      "cnt: 37\n",
      "cnt: 38\n",
      "cnt: 39\n",
      "cnt: 40\n",
      "cnt: 41\n",
      "cnt: 42\n",
      "cnt: 43\n",
      "cnt: 44\n",
      "cnt: 45\n",
      "cnt: 46\n",
      "cnt: 47\n",
      "cnt: 48\n",
      "cnt: 49\n",
      "cnt: 50\n",
      "cnt: 51\n",
      "cnt: 52\n",
      "cnt: 53\n",
      "cnt: 54\n",
      "cnt: 55\n"
     ]
    }
   ],
   "source": [
    "import traceback\n",
    "import numpy as np\n",
    "\n",
    "# define a video capture object \n",
    "#vid = cv2.VideoCapture(\"/media/cristian/12FF1F6D0CD48422/Research/Gloss/Gloss/Datasets/wlasl-complete/videos/17165.mp4\")#68508.mp4\")#54563.mp4\")#68914.mp4\") \n",
    "\n",
    "#vid = cv2.VideoCapture(\"/media/cristian/12FF1F6D0CD48422/Research/Gloss/Gloss/Datasets/WLASL/wlasl-complete-21k/videos/17165.mp4\")\n",
    "#vid = cv2.VideoCapture(\"/data/cristian/paper_2025/WLASL_videos/WLASL/videos/17165.mp4\")\n",
    "\n",
    "#vid = cv2.VideoCapture(\"../../data/videos/GIVE_24649.mp4\")\n",
    "vid = cv2.VideoCapture(\"../../data/videos/17165.mp4\")\n",
    "\n",
    "#vid = cv2.VideoCapture(0)\n",
    "\n",
    "\n",
    "cv2.namedWindow(\"frame\", cv2.WINDOW_NORMAL) \n",
    "\n",
    "list_keypoints = []\n",
    "list_scores = []\n",
    "cnt = 0\n",
    "\n",
    "while(True): \n",
    "    \n",
    "    ret, frame = vid.read() \n",
    "    if ret is None or frame is None:\n",
    "        break\n",
    "    \n",
    "    frame = cv2.resize(frame,(640,480))\n",
    "    #\"\"\"\n",
    "    try:\n",
    "        frame_rgb = cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        keypoints, scores  = model.predict(frame_rgb)\n",
    "        list_keypoints.append(keypoints[0,:,:])\n",
    "        list_scores.append(scores[0,:])\n",
    "\n",
    "\n",
    "        frame_original = draw_skeleton(copy.deepcopy(frame), keypoints, scores, kpt_thr=kpt_thr,\n",
    "                                    line_width=1,radius=1)\n",
    "        score_value = np.round(scores[0,99],2)\n",
    "        key_p1 = np.round(keypoints[0,99,0],2)\n",
    "        key_p2 = np.round(keypoints[0,99,1],2)\n",
    "        cnt+=1       \n",
    "        print(\"cnt:\",cnt)\n",
    "\n",
    "        frame_original = cv2.putText(frame_original, \n",
    "                    f'point 99:i={cnt},s={str(score_value)[:4]},k={str(key_p1)[:6]},{str(key_p2)[:6]}', \n",
    "                    (10, 30) ,cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 0) , 2, cv2.LINE_AA) \n",
    "        cv2.imshow('frame', frame_original) \n",
    "        #cv2.waitKey()\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'): \n",
    "            break\n",
    "    except:\n",
    "        print(traceback.format_exc())\n",
    "        break\n",
    "    #\"\"\"\n",
    "# After the loop release the cap object \n",
    "vid.release() \n",
    "# Destroy all the windows \n",
    "cv2.destroyAllWindows() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((55, 133, 2), (55, 133))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_keypoints = np.array(list_keypoints)\n",
    "list_scores = np.array(list_scores)\n",
    "list_keypoints.shape,list_scores.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install padasip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'padasip'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpadasip\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfilters\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AdaptiveFilter,FilterNLMS\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minitialize_adaptive_filter\u001b[39m(num_keypoints,mu\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m):\n\u001b[1;32m      6\u001b[0m     filters \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'padasip'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from padasip.filters import AdaptiveFilter,FilterNLMS\n",
    "\n",
    "def initialize_adaptive_filter(num_keypoints,mu=0.9):\n",
    "    filters = []\n",
    "    for _ in range(num_keypoints):\n",
    "        #filter_x = AdaptiveFilter(n=1, mu=0.00005, w=\"zeros\")\n",
    "        #filter_y = AdaptiveFilter(n=1, mu=0.00005, w=\"zeros\")\n",
    "        \n",
    "        filter_x = FilterNLMS(2, mu=mu,w=\"random\")\n",
    "        filter_y = FilterNLMS(2, mu=mu,w=\"random\")\n",
    "        filter_z = FilterNLMS(2, mu=mu,w=\"random\")\n",
    "\n",
    "        filters.append((filter_x, filter_y,filter_z))\n",
    "    return filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install filterpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeypointMemory:\n",
    "    def __init__(self, window=10):\n",
    "        self.window = window\n",
    "        self.last_high_score_frame = -np.inf\n",
    "        self.last_high_score_kp = None\n",
    "        self.last_high_score_value = 0\n",
    "\n",
    "    def update(self, frame_idx, keypoint, score, high_thresh):\n",
    "        if score >= high_thresh:\n",
    "            self.last_high_score_frame = frame_idx\n",
    "            self.last_high_score_kp = keypoint\n",
    "            self.last_high_score_value = score\n",
    "\n",
    "    def get(self, frame_idx):\n",
    "        if frame_idx - self.last_high_score_frame <= self.window:\n",
    "            return self.last_high_score_kp, self.last_high_score_value\n",
    "        else:\n",
    "            return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def track_keypoints(t, keypoints, scores, adaptive_filters, memories,\n",
    "                    low=0.25, high=0.5, interp_alpha=0.5):\n",
    "    keypoints_pred = np.zeros_like(keypoints)\n",
    "    scores_pred    = np.zeros_like(scores)\n",
    "\n",
    "    for i in range(len(keypoints)):\n",
    "        x, y = keypoints[i]\n",
    "        score = scores[i]\n",
    "        filter_x, filter_y, filter_s = adaptive_filters[i]\n",
    "        memory = memories[i]\n",
    "\n",
    "        input_vec = np.array([t, int(score > high)])\n",
    "\n",
    "        # Predicción del filtro\n",
    "        pred_x = filter_x.predict(x=input_vec)\n",
    "        pred_y = filter_y.predict(x=input_vec)\n",
    "        pred_s = filter_s.predict(x=input_vec)\n",
    "\n",
    "        if score >= high:\n",
    "            # Score confiable: usar directamente\n",
    "            filtered_x, filtered_y, filtered_s = x, y, score\n",
    "            memory.update(t, [x, y], score, high)\n",
    "            # Adaptación del filtro con observación actual\n",
    "            filter_x.adapt(x=input_vec, d=filtered_x)\n",
    "            filter_y.adapt(x=input_vec, d=filtered_y)\n",
    "            filter_s.adapt(x=input_vec, d=filtered_s)\n",
    "            \n",
    "        elif score < low:\n",
    "            # Score bajo: ¿puedo mantener último bueno?\n",
    "            remembered_kp, remembered_score = memory.get(t)\n",
    "            if remembered_kp is not None:\n",
    "                filtered_x, filtered_y = remembered_kp\n",
    "                remembered_score= remembered_score - (0.02 if remembered_score>0 else 0)\n",
    "                filtered_s = remembered_score\n",
    "            else:\n",
    "                # No tengo buena memoria reciente: usar predicción\n",
    "                filtered_x, filtered_y, filtered_s = pred_x, pred_y, pred_s\n",
    "            # Adaptación del filtro con observación actual\n",
    "            filter_x.adapt(x=input_vec, d=x)\n",
    "            filter_y.adapt(x=input_vec, d=y)\n",
    "            filter_s.adapt(x=input_vec, d=score)\n",
    "        else:\n",
    "            # Interpolación media\n",
    "            filtered_x = interp_alpha * x + (1 - interp_alpha) * pred_x\n",
    "            filtered_y = interp_alpha * y + (1 - interp_alpha) * pred_y\n",
    "            filtered_s = interp_alpha * score + (1 - interp_alpha) * pred_s\n",
    "            memory.update(t, [filtered_x, filtered_y], filtered_s, high)\n",
    "    \n",
    "            # Adaptación del filtro con observación actual\n",
    "            filter_x.adapt(x=input_vec, d=filtered_x)\n",
    "            filter_y.adapt(x=input_vec, d=filtered_y)\n",
    "            filter_s.adapt(x=input_vec, d=filtered_s)\n",
    "\n",
    "        #filter_x.adapt(x=input_vec, d=x)\n",
    "        #filter_y.adapt(x=input_vec, d=y)\n",
    "        #filter_s.adapt(x=input_vec, d=score)\n",
    "        keypoints_pred[i] = [filtered_x, filtered_y]\n",
    "        scores_pred[i] = filtered_s\n",
    "\n",
    "    return keypoints_pred, scores_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeypointMemory:\n",
    "    def __init__(self, window=10):\n",
    "        self.window = window\n",
    "        self.last_frame = -np.inf\n",
    "        self.last_pred_kp = None\n",
    "        self.last_pred_score = 0\n",
    "\n",
    "    def update(self, frame_idx, pred_kp, pred_score):\n",
    "        self.last_frame = frame_idx\n",
    "        self.last_pred_kp = pred_kp\n",
    "        self.last_pred_score = pred_score\n",
    "\n",
    "    def get(self, frame_idx):\n",
    "        if frame_idx - self.last_frame <= self.window:\n",
    "            return self.last_pred_kp, self.last_pred_score\n",
    "        else:\n",
    "            return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def track_keypoints(t, keypoints, scores, adaptive_filters, memories,\n",
    "                    low=0.25, high=0.5, interp_alpha=0.5):\n",
    "    keypoints_pred = np.zeros_like(keypoints)\n",
    "    scores_pred    = np.zeros_like(scores)\n",
    "\n",
    "    for i in range(len(keypoints)):\n",
    "        x, y = keypoints[i]\n",
    "        score = scores[i]\n",
    "        filter_x, filter_y, filter_s = adaptive_filters[i]\n",
    "        memory = memories[i]\n",
    "\n",
    "        score_vec = score\n",
    "        if score>high:\n",
    "            score_vec = 1\n",
    "        else:\n",
    "            score_vec = score/high\n",
    "        input_vec = np.array([t, score_vec])#int(score > high)])\n",
    "\n",
    "        # Predicción del filtro\n",
    "        pred_x = filter_x.predict(x=input_vec)\n",
    "        pred_y = filter_y.predict(x=input_vec)\n",
    "        pred_s = filter_s.predict(x=input_vec)\n",
    "\n",
    "        if score >= high:\n",
    "            # Confianza alta → usar valor observado\n",
    "            filtered_x, filtered_y, filtered_s = x, y, score\n",
    "\n",
    "        elif score < low:\n",
    "            # Confianza baja → intentar usar memoria si existe\n",
    "            remembered_kp, remembered_score = memory.get(t)\n",
    "            if remembered_kp is not None:\n",
    "                filtered_x, filtered_y = remembered_kp\n",
    "                filtered_s = max(0.0, remembered_score - 0.05)\n",
    "            else:\n",
    "                # Sin memoria válida → usar predicción\n",
    "                filtered_x, filtered_y, filtered_s = 0,0,0 #pred_x, pred_y, pred_s\n",
    "\n",
    "        else:\n",
    "            # Score medio → interpolar entre predicción y observación\n",
    "            filtered_x = interp_alpha * x + (1 - interp_alpha) * pred_x\n",
    "            filtered_y = interp_alpha * y + (1 - interp_alpha) * pred_y\n",
    "            filtered_s = interp_alpha * score + (1 - interp_alpha) * pred_s\n",
    "            print(\"x:\",x,\"pred_x:\",pred_x,\"filtered_x:\",filtered_x)\n",
    "        # Adaptar el filtro con observación (x, y, score), no el filtrado\n",
    "        filter_x.adapt(x=input_vec, d=x)\n",
    "        filter_y.adapt(x=input_vec, d=y)\n",
    "        filter_s.adapt(x=input_vec, d=score)\n",
    "\n",
    "        # ✅ Solo actualizar la memoria si el score no es muy bajo\n",
    "        if filtered_s >= low:\n",
    "            memory.update(t, [filtered_x, filtered_y], filtered_s)\n",
    "\n",
    "        # Guardar resultados\n",
    "        keypoints_pred[i] = [filtered_x, filtered_y]\n",
    "        scores_pred[i] = filtered_s\n",
    "\n",
    "    return keypoints_pred, scores_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_keypoints = np.array(list_keypoints)\n",
    "list_scores = np.array(list_scores)\n",
    "list_keypoints.shape,list_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id in [90,91,121]:    \n",
    "    print(id)\n",
    "    # Inicializar los filtros adaptativos para cada keypoint\n",
    "    num_keypoints = 10  # o 135*2 = 270 points\n",
    "    adaptive_filters = initialize_adaptive_filter(num_keypoints,mu=0.5)\n",
    "     \n",
    "    memories = [KeypointMemory(window=5) for _ in range(num_keypoints)]\n",
    "    \n",
    "    \n",
    "    # Coordinates x,y of index 99 and scores of pose estimation model\n",
    "    points_x      = list_keypoints[:,id,0] \n",
    "    points_y      = list_keypoints[:,id,1]\n",
    "    scores_points = list_scores[:,id]\n",
    "    \n",
    "    points_x_pred      = np.zeros_like(points_x)\n",
    "    points_y_pred      = np.zeros_like(points_y)\n",
    "    scores_points_pred = np.zeros_like(scores_points)\n",
    "    \n",
    "    # Simulation of filtering of coordinates frame by frame\n",
    "    for i in range(len(points_x)):\n",
    "        keyp_aux   = [[points_x[i],points_y[i]]]\n",
    "        scores_aux = [scores_points[i]]\n",
    "        \n",
    "        # filtering data (TO DO)\n",
    "        #print(\"keyp_aux.shape:\",np.array(keyp_aux).shape) # (1,2)\n",
    "        #print(\"scores_aux.shape:\",np.array(scores_aux).shape) # (1,)\n",
    "        keyp_pred,scores_pred = track_keypoints(i,keyp_aux,scores_aux, adaptive_filters,memories)#,\n",
    "                                               #low=2.5, high=4.5) #RTMPOSE low=1, high=2.5\n",
    "        \n",
    "        # save data predicted\n",
    "        points_x_pred[i] = keyp_pred[0][0]\n",
    "        points_y_pred[i] = keyp_pred[0][1]\n",
    "        scores_points_pred[i] = scores_pred[0]\n",
    "    \n",
    "    \n",
    "    n = len(points_x)\n",
    "    fig = plt.figure(figsize=(15,5))\n",
    "    \n",
    "    plt.subplot(3,1,1)\n",
    "    plt.plot(points_x, '-ob', label='Original X Points')\n",
    "    plt.plot(points_x_pred, '-or', alpha=0.5, label='Filtered X Points')\n",
    "    print(points_x_pred[32:36])\n",
    "    plt.xticks(np.arange(0, n, step=2))\n",
    "    plt.xlabel('Point Index')\n",
    "    plt.ylabel('X Coordinate')\n",
    "    plt.title('Pose Signal Filtering - X Coordinate')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(3,1,2)\n",
    "    plt.plot(points_y, '-ob', label='Original Y Points')\n",
    "    plt.plot(points_y_pred, '-or', alpha=0.5, label='Filtered Y Points')\n",
    "    plt.xticks(np.arange(0, n, step=2))\n",
    "    plt.xlabel('Point Index')\n",
    "    plt.ylabel('Y Coordinate')\n",
    "    plt.title('Pose Signal Filtering - Y Coordinate')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(3,1,3)\n",
    "    plt.plot(scores_points, '-ob', label='Original Scores')\n",
    "    plt.plot(scores_points_pred, '-or', alpha=0.5, label='Filtered Scores')\n",
    "    #plt.plot([0, len(points_x)], [0.5, 0.5], '-g', label='Score Threshold')\n",
    "    plt.plot([0, len(points_x)], [5, 5], '-g', label='Score Threshold')\n",
    "    plt.xticks(np.arange(0, n, step=2))\n",
    "    plt.xlabel('Point Index')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Score Signal Filtering')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "kpt_thr = 0.2\n",
    "\n",
    "# define a video capture object \n",
    "#vid = cv2.VideoCapture(\"/media/cristian/12FF1F6D0CD48422/Research/Gloss/Gloss/Datasets/wlasl-complete/videos/17165.mp4\")#68508.mp4\")#54563.mp4\")#68914.mp4\") \n",
    "\n",
    "\n",
    "num_keypoints = 133\n",
    "adaptive_filters = initialize_adaptive_filter(num_keypoints,mu=1.05)\n",
    "memories = [KeypointMemory(window=5) for _ in range(num_keypoints)]\n",
    "\n",
    "\n",
    "#vid = cv2.VideoCapture(\"/media/cristian/12FF1F6D0CD48422/Research/Gloss/Gloss/Datasets/WLASL/wlasl-complete-21k/videos/17165.mp4\")\n",
    "vid = cv2.VideoCapture(\"../../data/videos/17165.mp4\")\n",
    "\n",
    "#vid = cv2.VideoCapture(0)\n",
    "cv2.namedWindow(\"frame\", cv2.WINDOW_NORMAL) \n",
    "\n",
    "list_keypoints_new = []\n",
    "list_scores_new = []\n",
    "cnt = 0\n",
    "\n",
    "while(True): \n",
    "    \n",
    "    ret, frame = vid.read() \n",
    "    if ret is None or frame is None:\n",
    "        break\n",
    "    \n",
    "    frame = cv2.resize(frame,(640,480))\n",
    "    \n",
    "    try:\n",
    "        frame_rgb = cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        \n",
    "        #POSE ESTIMATION MODEL\n",
    "        keypoints, scores  = model.predict(frame_rgb)\n",
    "        list_keypoints_new.append(keypoints[0,:,:])\n",
    "        list_scores_new.append(scores[0,:])\n",
    "        \n",
    "        # FILTERING\n",
    "        keypoints_filtered       = np.zeros_like([[[0,0] for i in range(num_keypoints)]],dtype=float)\n",
    "        scores_filtered          = np.zeros_like([[0 for i in range(num_keypoints)]],dtype=float)\n",
    "        keyp_pred,scores_pred  = track_keypoints(cnt,keypoints[0,:,:],scores[0,:], adaptive_filters,memories)            \n",
    "        keypoints_filtered[0]  = keyp_pred\n",
    "        scores_filtered[0]     = scores_pred\n",
    "        #print(scores_pred[121])\n",
    "        \n",
    "        #DRAWING        \n",
    "        frame_original = draw_skeleton(copy.deepcopy(frame), keypoints_filtered, scores_filtered, kpt_thr=kpt_thr,\n",
    "                                    line_width=2,radius=2)\n",
    "        \n",
    "       \n",
    "        score_value = np.round(scores[0,121],2)\n",
    "        key_p1 = np.round(keypoints[0,121,0],2)\n",
    "        key_p2 = np.round(keypoints[0,121,1],2)\n",
    "        cnt+=1        \n",
    "\n",
    "        frame_original = cv2.putText(frame_original, \n",
    "                    f'point 121:i={cnt},s={str(score_value)[:4]},k={str(key_p1)[:6]},{str(key_p2)[:6]}', \n",
    "                    (10, 30) ,cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 0) , 2, cv2.LINE_AA) \n",
    "\n",
    "        score_value = np.round(scores_filtered[0,121],2)\n",
    "        key_p1 = np.round(keypoints_filtered[0,121,0],2)\n",
    "        key_p2 = np.round(keypoints_filtered[0,121,1],2)\n",
    "        #cnt+=1        \n",
    "\n",
    "        frame_original = cv2.putText(frame_original, \n",
    "                    f'pred point 121:i={cnt},s={str(score_value)[:4]},k={str(key_p1)[:6]},{str(key_p2)[:6]}', \n",
    "                    (10, 60) ,cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 0) , 2, cv2.LINE_AA) \n",
    "        \n",
    "        cv2.imshow('frame', frame_original) \n",
    "        #cv2.waitKey()\n",
    "\n",
    "        # DELAY TO SEE THE VIDEO SLOWLY\n",
    "        time.sleep(0.2)\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'): \n",
    "            break\n",
    "    except:\n",
    "        print(traceback.format_exc())\n",
    "        break\n",
    "    \n",
    "# After the loop release the cap object \n",
    "vid.release() \n",
    "# Destroy all the windows \n",
    "cv2.destroyAllWindows() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
